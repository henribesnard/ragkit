# RAGKIT v2 ‚Äî Roadmap Incr√©mental & Plan d'Impl√©mentation

## Philosophie

Chaque **√âtape** = un domaine fonctionnel autonome, testable, validable.
On ne passe √† l'√âtape N+1 que lorsque l'√âtape N est **installable, testable, et valid√©e**.
La version actuelle est archiv√©e dans une branche `legacy/v1`.

---

# PARTIE 1 ‚Äî ROADMAP COMPLET (13 √âtapes)

```
√âtape 1  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  INGESTION & PREPROCESSING
√âtape 2  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  CHUNKING
√âtape 3  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  EMBEDDING
√âtape 4  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë  BASE VECTORIELLE
√âtape 5  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà  RECHERCHE S√âMANTIQUE
√âtape 6  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  RECHERCHE LEXICALE
√âtape 7  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  RECHERCHE HYBRIDE
√âtape 8  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  RERANKING
√âtape 9  ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  LLM / G√âN√âRATION
√âtape 10 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  CACHE & PERFORMANCE
√âtape 11 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  MONITORING & EVALUATION
√âtape 12 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  S√âCURIT√â & COMPLIANCE
√âtape 13 ‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  MISE √Ä JOUR & MAINTENANCE
```

---

## √âtape 1 ‚Äî INGESTION & PREPROCESSING
**Livrable** : Pipeline d'ingestion qui prend un dossier de documents, les parse, les pr√©traite, et produit des `Document` propres avec m√©tadonn√©es enrichies.
**Validation** : CLI `ragkit ingest ./docs` ‚Üí affiche les documents pars√©s avec m√©tadonn√©es.
**Estimation** : 5-7 jours

## √âtape 2 ‚Äî CHUNKING
**Livrable** : Module de d√©coupage qui prend les `Document` de l'√âtape 1 et produit des `Chunk` avec m√©tadonn√©es h√©rit√©es + enrichies.
**Validation** : CLI `ragkit ingest ./docs --show-chunks` ‚Üí affiche les chunks avec stats.
**Estimation** : 4-5 jours

## √âtape 3 ‚Äî EMBEDDING
**Livrable** : Module d'embedding multi-provider qui vectorise les chunks.
**Validation** : CLI `ragkit embed` ‚Üí vectorise les chunks, affiche dimensions et temps.
**Estimation** : 4-5 jours

## √âtape 4 ‚Äî BASE DE DONN√âES VECTORIELLE
**Livrable** : Adapteurs vectorstore (ChromaDB, Qdrant) avec insertion, recherche, filtrage.
**Validation** : CLI `ragkit index` ‚Üí ins√®re les embeddings, `ragkit search "query"` ‚Üí r√©sultats.
**Estimation** : 5-6 jours

## √âtape 5 ‚Äî RECHERCHE S√âMANTIQUE
**Livrable** : Retriever s√©mantique avec query processing, MMR, filtres metadata.
**Validation** : CLI `ragkit search --mode semantic "query"` ‚Üí r√©sultats scor√©s et diversifi√©s.
**Estimation** : 3-4 jours

## √âtape 6 ‚Äî RECHERCHE LEXICALE
**Livrable** : Retriever BM25 aliment√© automatiquement depuis le vectorstore, avec tokenization configurable.
**Validation** : CLI `ragkit search --mode lexical "query"` ‚Üí r√©sultats BM25 non-vides.
**Estimation** : 3-4 jours

## √âtape 7 ‚Äî RECHERCHE HYBRIDE
**Livrable** : Fusion s√©mantique + lexicale avec RRF, weighted_sum, normalisation.
**Validation** : CLI `ragkit search --mode hybrid "query"` ‚Üí r√©sultats fusionn√©s.
**Estimation** : 3-4 jours

## √âtape 8 ‚Äî RERANKING
**Livrable** : Reranker multi-provider (Cohere, cross-encoder local) post-retrieval.
**Validation** : CLI `ragkit search --rerank "query"` ‚Üí r√©sultats r√©ordonn√©s.
**Estimation** : 3-4 jours

## √âtape 9 ‚Äî LLM / G√âN√âRATION
**Livrable** : Pipeline RAG complet : retrieval ‚Üí context ‚Üí LLM ‚Üí r√©ponse avec citations.
**Validation** : CLI `ragkit query "question"` ‚Üí r√©ponse avec sources cit√©es.
**Estimation** : 5-7 jours

## √âtape 10 ‚Äî CACHE & PERFORMANCE
**Livrable** : Cache requ√™tes, cache embeddings, async processing, warmup.
**Validation** : Benchmark avant/apr√®s sur 100 requ√™tes, latence p50/p95.
**Estimation** : 3-4 jours

## √âtape 11 ‚Äî MONITORING & EVALUATION
**Livrable** : M√©triques retrieval (precision@k, recall@k, MRR), logging, feedback.
**Validation** : Dashboard m√©triques, `ragkit eval` sur un dataset de test.
**Estimation** : 4-5 jours

## √âtape 12 ‚Äî S√âCURIT√â & COMPLIANCE
**Livrable** : Contr√¥le d'acc√®s, PII detection/redaction, filtres contenu.
**Validation** : Tests d'intrusion, v√©rification RGPD sur documents de test.
**Estimation** : 4-5 jours

## √âtape 13 ‚Äî MISE √Ä JOUR & MAINTENANCE
**Livrable** : Indexation incr√©mentale, versioning, auto-refresh.
**Validation** : Ajout/suppression de documents sans r√©indexation totale.
**Estimation** : 3-4 jours

---

# PARTIE 2 ‚Äî STRUCTURE DE M√âTADONN√âES PAR D√âFAUT

Cette structure est le **socle** de toute l'application. Elle est d√©finie √† l'√âtape 1 et utilis√©e par toutes les √©tapes suivantes.

## Sch√©ma `DocumentMetadata`

```yaml
# ‚îÄ‚îÄ‚îÄ HI√âRARCHIE ORGANISATIONNELLE (configur√©e par l'utilisateur) ‚îÄ‚îÄ‚îÄ
tenant: "acme-corp"              # Organisation / client
domain: "engineering"            # Domaine m√©tier
subdomain: "backend"             # Sous-domaine

# ‚îÄ‚îÄ‚îÄ IDENTIFICATION DOCUMENT (d√©tect√© automatiquement + modifiable) ‚îÄ‚îÄ‚îÄ
document_id: "doc_a1b2c3"       # ID unique g√©n√©r√©
title: "Guide API REST"         # Extrait du H1 ou nom de fichier
author: "Jean Dupont"           # Extrait des m√©tadonn√©es PDF/DOCX
source: "api-guide.pdf"         # Nom du fichier source
source_path: "./docs/api/"      # Chemin relatif du fichier
source_type: "pdf"              # Type de fichier (pdf, docx, md, txt, html, csv)
source_url: ""                  # URL d'origine si applicable
mime_type: "application/pdf"    # MIME type d√©tect√©

# ‚îÄ‚îÄ‚îÄ TEMPORALIT√â (d√©tect√© automatiquement) ‚îÄ‚îÄ‚îÄ
created_at: "2025-06-15"        # Date de cr√©ation du document
modified_at: "2026-01-20"       # Date de derni√®re modification
ingested_at: "2026-02-12T..."   # Timestamp d'ingestion dans RAGKIT
version: "1.0"                  # Version du document

# ‚îÄ‚îÄ‚îÄ CONTENU (d√©tect√© automatiquement) ‚îÄ‚îÄ‚îÄ
language: "fr"                  # Langue d√©tect√©e (ISO 639-1)
page_count: 42                  # Nombre de pages
word_count: 12500               # Nombre de mots
has_tables: true                # Contient des tableaux
has_images: true                # Contient des images
has_code: false                 # Contient des blocs de code
encoding: "utf-8"              # Encodage d√©tect√©

# ‚îÄ‚îÄ‚îÄ CLASSIFICATION (modifiable par l'utilisateur) ‚îÄ‚îÄ‚îÄ
tags: ["api", "rest", "auth"]   # Tags libres
category: "technical"           # Cat√©gorie pr√©d√©finie
confidentiality: "internal"     # public | internal | confidential | secret
status: "published"             # draft | review | published | archived

# ‚îÄ‚îÄ‚îÄ PARSING (rempli par le syst√®me) ‚îÄ‚îÄ‚îÄ
parser_engine: "unstructured"   # Moteur utilis√©
ocr_applied: false              # OCR a √©t√© n√©cessaire
parsing_quality: 0.95           # Score de qualit√© du parsing (0-1)
parsing_warnings: []            # Avertissements √©ventuels
```

## Sch√©ma `ChunkMetadata` (h√©rit√© + enrichi √† l'√âtape 2)

```yaml
# ‚îÄ‚îÄ‚îÄ H√âRIT√â DU DOCUMENT (copi√© automatiquement) ‚îÄ‚îÄ‚îÄ
document_id: "doc_a1b2c3"
tenant: "acme-corp"
domain: "engineering"
title: "Guide API REST"
source: "api-guide.pdf"
language: "fr"
tags: ["api", "rest", "auth"]

# ‚îÄ‚îÄ‚îÄ SP√âCIFIQUE AU CHUNK (g√©n√©r√© par le chunker) ‚îÄ‚îÄ‚îÄ
chunk_id: "doc_a1b2c3-chunk-007"
chunk_index: 7                  # Position dans le document
total_chunks: 23                # Nombre total de chunks du document
chunk_strategy: "semantic"      # Strat√©gie utilis√©e
chunk_size_tokens: 487          # Taille en tokens
chunk_size_chars: 2134          # Taille en caract√®res

# ‚îÄ‚îÄ‚îÄ CONTEXTE STRUCTUREL ‚îÄ‚îÄ‚îÄ
page_number: 12                 # Page d'origine
section_title: "Authentication" # Titre de section parent
heading_path: ["API", "Auth"]   # Fil d'Ariane des headings
paragraph_index: 3              # Index du paragraphe dans la section

# ‚îÄ‚îÄ‚îÄ RELATIONS ‚îÄ‚îÄ‚îÄ
previous_chunk_id: "...-chunk-006"
next_chunk_id: "...-chunk-008"
parent_chunk_id: null           # Pour le parent-child chunking
```

---

# PARTIE 3 ‚Äî PLAN D'IMPL√âMENTATION D√âTAILL√â : √âTAPE 1

# √âTAPE 1 : INGESTION & PREPROCESSING

**Objectif** : Pipeline robuste qui transforme tout type de document en objets `Document` normalis√©s avec des m√©tadonn√©es compl√®tes et un texte propre.

**Branche Git** : `v2/step-01-ingestion`

---

## Phase 1.1 ‚Äî Fondations (Jour 1)

### 1.1.1 Structure du projet

```
ragkit/
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ ragkit/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Document, Chunk, DocumentMetadata
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schema.py          # Pydantic models pour toute la config
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ defaults.py        # Valeurs par d√©faut
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py        # IngestionPipeline orchestrateur
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parsers/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py        # BaseParser (interface)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf.py         # PDFParser
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ docx.py        # DocxParser
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown.py    # MarkdownParser
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text.py        # TextParser
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html.py        # HTMLParser
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csv_parser.py  # CSVParser
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ factory.py     # parser_factory(source_type) ‚Üí Parser
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py    # PreprocessingPipeline
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ normalizer.py  # Unicode, casse, ponctuation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cleaner.py     # URLs, espaces, caract√®res sp√©ciaux
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ language.py    # D√©tection de langue
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dedup.py       # D√©duplication (exact, fuzzy, semantic)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ metadata/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ extractor.py   # Extraction auto des m√©tadonn√©es
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ enricher.py    # Enrichissement (langue, stats, etc.)
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ schema.py      # DocumentMetadata Pydantic model
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hashing.py         # Fingerprinting pour dedup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_utils.py      # Scan r√©pertoire, d√©tection type
‚îÇ   ‚îî‚îÄ‚îÄ cli/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ ingest.py          # Commande CLI ragkit ingest
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_models.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_parsers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_preprocessing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_metadata/
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_ingestion_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/
‚îÇ       ‚îú‚îÄ‚îÄ sample.pdf
‚îÇ       ‚îú‚îÄ‚îÄ sample.docx
‚îÇ       ‚îú‚îÄ‚îÄ sample.md
‚îÇ       ‚îú‚îÄ‚îÄ sample.txt
‚îÇ       ‚îú‚îÄ‚îÄ sample.html
‚îÇ       ‚îî‚îÄ‚îÄ sample.csv
‚îî‚îÄ‚îÄ ragkit.yaml                 # Config par d√©faut
```

### 1.1.2 Mod√®les de donn√©es (`ragkit/models.py`)

```python
from __future__ import annotations
from datetime import datetime
from pydantic import BaseModel, Field
from typing import Any
import uuid


class DocumentMetadata(BaseModel):
    """Structure de m√©tadonn√©es par d√©faut, large et extensible."""

    # ‚îÄ‚îÄ‚îÄ HI√âRARCHIE ORGANISATIONNELLE ‚îÄ‚îÄ‚îÄ
    tenant: str = "default"
    domain: str = ""
    subdomain: str = ""

    # ‚îÄ‚îÄ‚îÄ IDENTIFICATION ‚îÄ‚îÄ‚îÄ
    document_id: str = Field(default_factory=lambda: f"doc_{uuid.uuid4().hex[:8]}")
    title: str = ""
    author: str = ""
    source: str = ""               # nom de fichier
    source_path: str = ""          # chemin relatif
    source_type: str = ""          # pdf, docx, md, txt, html, csv
    source_url: str = ""
    mime_type: str = ""

    # ‚îÄ‚îÄ‚îÄ TEMPORALIT√â ‚îÄ‚îÄ‚îÄ
    created_at: datetime | None = None
    modified_at: datetime | None = None
    ingested_at: datetime = Field(default_factory=datetime.utcnow)
    version: str = "1.0"

    # ‚îÄ‚îÄ‚îÄ CONTENU (auto-d√©tect√©) ‚îÄ‚îÄ‚îÄ
    language: str = ""             # ISO 639-1
    page_count: int = 0
    word_count: int = 0
    char_count: int = 0
    has_tables: bool = False
    has_images: bool = False
    has_code: bool = False
    encoding: str = "utf-8"

    # ‚îÄ‚îÄ‚îÄ CLASSIFICATION (user-editable) ‚îÄ‚îÄ‚îÄ
    tags: list[str] = Field(default_factory=list)
    category: str = ""
    confidentiality: str = "internal"  # public|internal|confidential|secret
    status: str = "published"          # draft|review|published|archived

    # ‚îÄ‚îÄ‚îÄ PARSING (syst√®me) ‚îÄ‚îÄ‚îÄ
    parser_engine: str = ""
    ocr_applied: bool = False
    parsing_quality: float = 1.0
    parsing_warnings: list[str] = Field(default_factory=list)

    # ‚îÄ‚îÄ‚îÄ EXTENSIBLE ‚îÄ‚îÄ‚îÄ
    custom: dict[str, Any] = Field(default_factory=dict)


class Document(BaseModel):
    """Document pars√© avec texte brut et m√©tadonn√©es."""

    id: str = Field(default_factory=lambda: f"doc_{uuid.uuid4().hex[:8]}")
    content: str                    # texte brut extrait
    raw_content: str = ""           # contenu original avant preprocessing
    metadata: DocumentMetadata = Field(default_factory=DocumentMetadata)

    # Sections structur√©es (optionnel, pour header_detection)
    sections: list[DocumentSection] = Field(default_factory=list)
    tables: list[TableData] = Field(default_factory=list)


class DocumentSection(BaseModel):
    """Section d√©tect√©e dans le document."""
    title: str = ""
    level: int = 0                 # 0=titre principal, 1=H2, 2=H3...
    content: str = ""
    page_number: int | None = None


class TableData(BaseModel):
    """Tableau extrait du document."""
    page_number: int | None = None
    headers: list[str] = Field(default_factory=list)
    rows: list[list[str]] = Field(default_factory=list)
    caption: str = ""
```

### 1.1.3 Configuration (`ragkit/config/schema.py` ‚Äî section ingestion)

```python
class OCRConfig(BaseModel):
    enabled: bool = False
    engine: str = "tesseract"      # tesseract | easyocr
    languages: list[str] = ["fra", "eng"]

class ParsingConfig(BaseModel):
    engine: str = "auto"           # auto | unstructured | docling | pypdf
    ocr: OCRConfig = Field(default_factory=OCRConfig)
    table_extraction: str = "auto" # auto | preserve | markdown | separate | none
    image_captioning: bool = False
    header_detection: bool = True

class PreprocessingConfig(BaseModel):
    lowercase: bool = False
    remove_punctuation: bool = False
    normalize_unicode: str = "NFKC"  # NFC | NFD | NFKC | NFKD | none
    remove_urls: bool = False
    language_detection: bool = True
    strip_extra_whitespace: bool = True

class DeduplicationConfig(BaseModel):
    enabled: bool = False
    strategy: str = "exact"        # exact | fuzzy | semantic
    threshold: float = 0.95

class MetadataDefaultsConfig(BaseModel):
    """Valeurs par d√©faut inject√©es dans tous les documents."""
    tenant: str = "default"
    domain: str = ""
    subdomain: str = ""
    confidentiality: str = "internal"
    tags: list[str] = Field(default_factory=list)

class SourceConfig(BaseModel):
    type: str = "local"
    path: str = "./data/documents"
    patterns: list[str] = ["*.pdf", "*.docx", "*.md", "*.txt"]
    recursive: bool = True

class IngestionConfig(BaseModel):
    sources: list[SourceConfig] = Field(default_factory=lambda: [SourceConfig()])
    parsing: ParsingConfig = Field(default_factory=ParsingConfig)
    preprocessing: PreprocessingConfig = Field(default_factory=PreprocessingConfig)
    deduplication: DeduplicationConfig = Field(default_factory=DeduplicationConfig)
    metadata_defaults: MetadataDefaultsConfig = Field(default_factory=MetadataDefaultsConfig)
```

**Crit√®re de validation Phase 1.1** :
- `pytest tests/unit/test_models.py` ‚Üí tous les mod√®les Pydantic valident et s√©rialisent

---

## Phase 1.2 ‚Äî Parsers (Jours 2-3)

### 1.2.1 Interface de base

```python
# ragkit/ingestion/parsers/base.py
from abc import ABC, abstractmethod
from ragkit.models import Document, DocumentMetadata

class BaseParser(ABC):
    """Interface commune pour tous les parsers."""

    @abstractmethod
    def parse(self, file_path: str, metadata: DocumentMetadata) -> Document:
        """Parse un fichier et retourne un Document."""
        ...

    @abstractmethod
    def supports(self, file_extension: str) -> bool:
        """Retourne True si ce parser supporte l'extension."""
        ...
```

### 1.2.2 Parsers √† impl√©menter

| Parser | Fichier | Lib principale | Auto-d√©tection m√©tadonn√©es |
|--------|---------|---------------|--------------------------|
| `PDFParser` | `pdf.py` | `pypdf` + `unstructured` | title (metadata PDF), author, created_at, page_count, has_images, has_tables |
| `DocxParser` | `docx.py` | `python-docx` | title, author, created_at, modified_at, word_count, has_tables, has_images |
| `MarkdownParser` | `markdown.py` | `markdown-it-py` | title (premier H1), headings structure, has_code |
| `TextParser` | `text.py` | stdlib | encoding detection, word_count |
| `HTMLParser` | `html.py` | `beautifulsoup4` | title (tag `<title>`), language (tag `<html lang>`), has_tables |
| `CSVParser` | `csv_parser.py` | `pandas` | column names, row_count, d√©tection s√©parateur |

### 1.2.3 Factory

```python
# ragkit/ingestion/parsers/factory.py
PARSER_MAP = {
    ".pdf": PDFParser,
    ".docx": DocxParser,
    ".doc": DocxParser,       # conversion .doc ‚Üí .docx via LibreOffice
    ".md": MarkdownParser,
    ".txt": TextParser,
    ".html": HTMLParser,
    ".htm": HTMLParser,
    ".csv": CSVParser,
}

def create_parser(file_path: str, config: ParsingConfig) -> BaseParser:
    ext = Path(file_path).suffix.lower()
    parser_class = PARSER_MAP.get(ext)
    if not parser_class:
        raise UnsupportedFormatError(f"Format {ext} non support√©")
    return parser_class(config)
```

### 1.2.4 OCR int√©gr√© dans PDFParser

```python
# Dans pdf.py
class PDFParser(BaseParser):
    def parse(self, file_path, metadata):
        doc = self._extract_text(file_path)

        # Si le texte est trop court vs nombre de pages ‚Üí probablement scann√©
        if self._needs_ocr(doc, file_path):
            if self.config.ocr.enabled:
                doc = self._apply_ocr(file_path)
                metadata.ocr_applied = True
            else:
                metadata.parsing_warnings.append("Document semble scann√© mais OCR d√©sactiv√©")

        return doc
```

**Crit√®re de validation Phase 1.2** :
- Chaque parser test√© avec un fichier fixture
- `pytest tests/unit/test_parsers/` ‚Üí 100% pass
- M√©tadonn√©es auto-d√©tect√©es correctement pour chaque format

---

## Phase 1.3 ‚Äî Metadata Extractor & Enricher (Jour 4)

### 1.3.1 Extraction automatique

```python
# ragkit/ingestion/metadata/extractor.py
class MetadataExtractor:
    """Extrait les m√©tadonn√©es depuis le fichier source."""

    def extract(self, file_path: str) -> DocumentMetadata:
        metadata = DocumentMetadata()

        # Infos fichier syst√®me
        path = Path(file_path)
        stat = path.stat()
        metadata.source = path.name
        metadata.source_path = str(path.parent)
        metadata.source_type = path.suffix.lstrip(".").lower()
        metadata.mime_type = mimetypes.guess_type(file_path)[0] or ""
        metadata.modified_at = datetime.fromtimestamp(stat.st_mtime)
        metadata.created_at = datetime.fromtimestamp(stat.st_ctime)

        return metadata
```

### 1.3.2 Enrichissement post-parsing

```python
# ragkit/ingestion/metadata/enricher.py
class MetadataEnricher:
    """Enrichit les m√©tadonn√©es apr√®s parsing du contenu."""

    def __init__(self, config: PreprocessingConfig):
        self.config = config

    def enrich(self, doc: Document) -> Document:
        meta = doc.metadata

        # Statistiques de contenu
        meta.word_count = len(doc.content.split())
        meta.char_count = len(doc.content)

        # D√©tection de langue
        if self.config.language_detection and doc.content:
            meta.language = detect_language(doc.content)

        # D√©tection de contenu sp√©cial
        meta.has_code = self._detect_code(doc.content)
        meta.has_tables = len(doc.tables) > 0

        # Titre fallback : premier heading ou nom de fichier
        if not meta.title:
            meta.title = self._extract_title(doc) or meta.source

        # Score qualit√©
        meta.parsing_quality = self._compute_quality_score(doc)

        return doc
```

### 1.3.3 Application des defaults utilisateur

```python
# ragkit/ingestion/metadata/enricher.py
    def apply_defaults(self, doc: Document, defaults: MetadataDefaultsConfig) -> Document:
        """Applique les valeurs par d√©faut de l'utilisateur."""
        if defaults.tenant:
            doc.metadata.tenant = defaults.tenant
        if defaults.domain:
            doc.metadata.domain = defaults.domain
        if defaults.subdomain:
            doc.metadata.subdomain = defaults.subdomain
        if defaults.tags:
            doc.metadata.tags = list(set(doc.metadata.tags + defaults.tags))
        if defaults.confidentiality:
            doc.metadata.confidentiality = defaults.confidentiality
        return doc
```

**Crit√®re de validation Phase 1.3** :
- Un PDF de test ‚Üí m√©tadonn√©es compl√®tes auto-d√©tect√©es (title, author, language, page_count, etc.)
- Les defaults utilisateur s'appliquent correctement
- `pytest tests/unit/test_metadata/` ‚Üí pass

---

## Phase 1.4 ‚Äî Preprocessing Pipeline (Jour 5)

### 1.4.1 Normalizer

```python
# ragkit/ingestion/preprocessing/normalizer.py
import unicodedata, re

class TextNormalizer:
    def __init__(self, config: PreprocessingConfig):
        self.config = config

    def normalize(self, text: str) -> str:
        # 1. Unicode normalization
        if self.config.normalize_unicode != "none":
            text = unicodedata.normalize(self.config.normalize_unicode, text)

        # 2. Extra whitespace
        if self.config.strip_extra_whitespace:
            text = re.sub(r'\s+', ' ', text).strip()
            text = re.sub(r'\n{3,}', '\n\n', text)

        # 3. URLs
        if self.config.remove_urls:
            text = re.sub(r'https?://\S+', '', text)

        # 4. Lowercase (optionnel ‚Äî attention, d√©sactiv√© par d√©faut)
        if self.config.lowercase:
            text = text.lower()

        # 5. Ponctuation
        if self.config.remove_punctuation:
            text = re.sub(r'[^\w\s]', '', text)

        return text
```

### 1.4.2 Deduplication

```python
# ragkit/ingestion/preprocessing/dedup.py
import hashlib

class Deduplicator:
    def __init__(self, config: DeduplicationConfig):
        self.config = config
        self._seen_hashes: set[str] = set()

    def is_duplicate(self, doc: Document) -> bool:
        if not self.config.enabled:
            return False

        if self.config.strategy == "exact":
            h = hashlib.sha256(doc.content.encode()).hexdigest()
            if h in self._seen_hashes:
                return True
            self._seen_hashes.add(h)
            return False

        elif self.config.strategy == "fuzzy":
            # MinHash / SimHash pour near-duplicates
            return self._fuzzy_check(doc)

        return False
```

### 1.4.3 Pipeline complet

```python
# ragkit/ingestion/preprocessing/pipeline.py
class PreprocessingPipeline:
    def __init__(self, config: PreprocessingConfig):
        self.normalizer = TextNormalizer(config)

    def process(self, doc: Document) -> Document:
        """Applique toutes les √©tapes de preprocessing."""
        doc.raw_content = doc.content  # sauvegarde de l'original
        doc.content = self.normalizer.normalize(doc.content)
        return doc
```

**Crit√®re de validation Phase 1.4** :
- Texte avec caract√®res Unicode bizarres ‚Üí normalis√© proprement
- Document en double ‚Üí d√©tect√© comme duplicate
- `pytest tests/unit/test_preprocessing/` ‚Üí pass

---

## Phase 1.5 ‚Äî Orchestrateur & CLI (Jour 6)

### 1.5.1 IngestionPipeline

```python
# ragkit/ingestion/pipeline.py
class IngestionPipeline:
    def __init__(self, config: IngestionConfig):
        self.config = config
        self.preprocessor = PreprocessingPipeline(config.preprocessing)
        self.metadata_extractor = MetadataExtractor()
        self.metadata_enricher = MetadataEnricher(config.preprocessing)
        self.deduplicator = Deduplicator(config.deduplication)

    def ingest(self, source_path: str) -> list[Document]:
        """Pipeline complet : scan ‚Üí parse ‚Üí metadata ‚Üí preprocess ‚Üí dedup."""
        documents = []
        files = scan_directory(source_path, self.config.sources[0].patterns,
                               self.config.sources[0].recursive)

        for file_path in files:
            try:
                # 1. Extraction m√©tadonn√©es fichier
                metadata = self.metadata_extractor.extract(file_path)

                # 2. Application des defaults utilisateur
                self.metadata_enricher.apply_defaults(metadata, self.config.metadata_defaults)

                # 3. Parsing
                parser = create_parser(file_path, self.config.parsing)
                doc = parser.parse(file_path, metadata)
                doc.metadata.parser_engine = parser.__class__.__name__

                # 4. Enrichissement m√©tadonn√©es post-parsing
                doc = self.metadata_enricher.enrich(doc)

                # 5. Preprocessing texte
                doc = self.preprocessor.process(doc)

                # 6. D√©duplication
                if self.deduplicator.is_duplicate(doc):
                    logger.info(f"Duplicate d√©tect√©, ignor√©: {file_path}")
                    continue

                documents.append(doc)
                logger.info(f"Ing√©r√©: {file_path} ‚Üí {doc.metadata.word_count} mots, "
                           f"lang={doc.metadata.language}")

            except Exception as e:
                logger.error(f"Erreur ingestion {file_path}: {e}")

        return documents
```

### 1.5.2 CLI

```python
# ragkit/cli/ingest.py
import click

@click.command()
@click.argument("path", default="./data/documents")
@click.option("--config", "-c", default="ragkit.yaml")
@click.option("--show-metadata", is_flag=True)
@click.option("--dry-run", is_flag=True)
def ingest(path, config, show_metadata, dry_run):
    """Ing√®re les documents depuis un dossier."""
    cfg = load_config(config)
    pipeline = IngestionPipeline(cfg.ingestion)
    documents = pipeline.ingest(path)

    click.echo(f"\n{'='*60}")
    click.echo(f"  Ingestion termin√©e : {len(documents)} documents")
    click.echo(f"{'='*60}")

    for doc in documents:
        m = doc.metadata
        click.echo(f"\n  üìÑ {m.source}")
        click.echo(f"     Titre    : {m.title}")
        click.echo(f"     Langue   : {m.language}")
        click.echo(f"     Mots     : {m.word_count}")
        click.echo(f"     Pages    : {m.page_count}")
        click.echo(f"     Qualit√©  : {m.parsing_quality:.0%}")
        if show_metadata:
            click.echo(f"     Metadata : {m.model_dump_json(indent=2)}")

    if not dry_run:
        # Sauvegarder les documents pour l'√âtape 2
        save_documents(documents, cfg.data_dir / "ingested")
```

**Crit√®re de validation Phase 1.5** :
- `ragkit ingest ./tests/fixtures/ --show-metadata` ‚Üí affiche les 6 documents de test avec m√©tadonn√©es compl√®tes
- Chaque format (PDF, DOCX, MD, TXT, HTML, CSV) pars√© correctement
- Les m√©tadonn√©es auto-d√©tect√©es sont exactes

---

## Phase 1.6 ‚Äî Tests d'int√©gration & Validation finale (Jour 7)

### Tests √† √©crire

```python
# tests/integration/test_ingestion_pipeline.py

class TestIngestionPipeline:
    def test_full_pipeline_mixed_formats(self, fixtures_dir):
        """Ing√®re un dossier avec tous les formats support√©s."""
        config = IngestionConfig(
            sources=[SourceConfig(path=str(fixtures_dir))],
            metadata_defaults=MetadataDefaultsConfig(
                tenant="test-corp", domain="engineering"
            ),
        )
        pipeline = IngestionPipeline(config)
        docs = pipeline.ingest(str(fixtures_dir))

        assert len(docs) >= 5  # au moins les 5 formats principaux
        for doc in docs:
            assert doc.metadata.tenant == "test-corp"
            assert doc.metadata.domain == "engineering"
            assert doc.metadata.language != ""
            assert doc.metadata.word_count > 0
            assert doc.metadata.source != ""

    def test_metadata_auto_detection_pdf(self, sample_pdf):
        """Les m√©tadonn√©es PDF sont extraites automatiquement."""
        ...

    def test_deduplication_exact(self, fixtures_dir):
        """Les documents identiques sont d√©tect√©s."""
        ...

    def test_preprocessing_unicode(self):
        """La normalisation Unicode fonctionne."""
        ...

    def test_ocr_fallback(self, scanned_pdf):
        """L'OCR se d√©clenche sur un PDF scann√©."""
        ...

    def test_metadata_defaults_applied(self):
        """Les defaults utilisateur sont inject√©s."""
        ...
```

### Checklist de validation finale √âtape 1

```
‚ñ° ragkit ingest ./docs ‚Üí parse tous les formats sans erreur
‚ñ° M√©tadonn√©es auto-d√©tect√©es : title, author, language, page_count, word_count
‚ñ° M√©tadonn√©es par d√©faut (tenant, domain) appliqu√©es
‚ñ° OCR se d√©clenche sur PDF scann√© quand activ√©
‚ñ° D√©duplication fonctionne (exact)
‚ñ° Preprocessing : unicode normalis√©, whitespace nettoy√©
‚ñ° CLI affiche un r√©sum√© clair
‚ñ° Documents s√©rialis√©s sur disque pour l'√âtape 2
‚ñ° pytest ‚Üí 100% pass (unit + integration)
‚ñ° Aucune d√©pendance sur les √©tapes suivantes (embedding, vectorstore, etc.)
```

---

## D√©pendances Python (√âtape 1 uniquement)

```toml
[project]
dependencies = [
    "pydantic>=2.0",
    "click>=8.0",
    "pypdf>=4.0",
    "python-docx>=1.0",
    "beautifulsoup4>=4.12",
    "markdown-it-py>=3.0",
    "pandas>=2.0",
    "langdetect>=1.0.9",
    "python-magic>=0.4",        # d√©tection MIME
    "structlog>=24.0",
    "pyyaml>=6.0",
]

[project.optional-dependencies]
ocr = [
    "pytesseract>=0.3",
    "easyocr>=1.7",
]
```

---

## R√©capitulatif √âtape 1

| Phase | Contenu | Dur√©e | Livrable |
|-------|---------|-------|----------|
| 1.1 | Mod√®les, config, structure projet | 1 jour | `models.py`, `schema.py` |
| 1.2 | 6 parsers + factory + OCR | 2 jours | Parsing de tous les formats |
| 1.3 | Metadata extractor + enricher | 1 jour | M√©tadonn√©es auto-d√©tect√©es |
| 1.4 | Preprocessing (normalisation, dedup) | 1 jour | Texte propre et normalis√© |
| 1.5 | Pipeline orchestrateur + CLI | 1 jour | `ragkit ingest` fonctionnel |
| 1.6 | Tests int√©gration + validation | 1 jour | 100% coverage, pipeline valid√© |
| **Total** | | **7 jours** | **√âtape 1 compl√®te et valid√©e** |
