# ============================================================================
# RAGKIT V1 - CONFIGURATION DE RÉFÉRENCE
# ============================================================================
# Ce fichier contient la configuration complète pour RAGKIT V1
# avec les agents par défaut (query_analyzer + response_generator)
# ============================================================================

version: "1.0"

# ============================================================================
# PROJECT - Métadonnées du projet
# ============================================================================
project:
  name: "my-ragkit-project"
  description: "Assistant RAG documentaire"
  environment: "development"  # development | staging | production

# ============================================================================
# INGESTION - Pipeline d'ingestion des documents
# ============================================================================
ingestion:
  # Sources de données (V1 : local uniquement)
  sources:
    - type: "local"
      path: "./data/documents"
      patterns:
        - "*.pdf"
        - "*.docx"
        - "*.md"
        - "*.txt"
      recursive: true
      
  # Configuration du parsing
  parsing:
    # Moteur de parsing
    engine: "auto"  # auto | unstructured | docling | pypdf
    
    # OCR pour PDFs scannés
    ocr:
      enabled: false
      engine: "tesseract"  # tesseract | easyocr
      languages: ["fra", "eng"]

  # Configuration du chunking
  chunking:
    # Stratégie de découpage
    strategy: "fixed"  # fixed | semantic
    
    # Paramètres pour stratégie "fixed"
    fixed:
      chunk_size: 512        # Taille en tokens
      chunk_overlap: 50      # Chevauchement entre chunks
      
    # Paramètres pour stratégie "semantic"  
    semantic:
      similarity_threshold: 0.85
      min_chunk_size: 100
      max_chunk_size: 1000
      embedding_model: "document_model"  # Référence au modèle d'embedding

  # Métadonnées extraites automatiquement
  metadata:
    extract:
      - "title"
      - "source_path"
      - "file_type"
      - "created_date"
    # Métadonnées custom ajoutées à tous les documents
    custom: {}

# ============================================================================
# EMBEDDING - Configuration des modèles d'embedding
# ============================================================================
embedding:
  # Modèle pour les documents
  document_model:
    provider: "openai"  # openai | ollama | cohere
    model: "text-embedding-3-small"
    api_key_env: "OPENAI_API_KEY"
    
    # Paramètres optionnels
    params:
      batch_size: 100
      dimensions: null  # null = dimensions par défaut du modèle
    
    # Cache des embeddings
    cache:
      enabled: true
      backend: "memory"  # memory | disk
      
  # Modèle pour les requêtes (peut être identique ou différent)
  query_model:
    provider: "openai"
    model: "text-embedding-3-small"
    api_key_env: "OPENAI_API_KEY"

# ============================================================================
# VECTOR STORE - Base de données vectorielle
# ============================================================================
vector_store:
  # Provider (V1 : qdrant ou chroma)
  provider: "qdrant"  # qdrant | chroma
  
  # Configuration Qdrant
  qdrant:
    mode: "memory"  # memory | local | cloud
    # Pour mode "local"
    path: "./data/qdrant"
    # Pour mode "cloud"
    url_env: "QDRANT_URL"
    api_key_env: "QDRANT_API_KEY"
    # Collection
    collection_name: "ragkit_documents"
    # Index
    distance_metric: "cosine"  # cosine | euclidean | dot
    
  # Configuration ChromaDB
  chroma:
    mode: "memory"  # memory | persistent
    # Pour mode "persistent"
    path: "./data/chroma"
    collection_name: "ragkit_documents"

# ============================================================================
# RETRIEVAL - Configuration de la recherche
# ============================================================================
retrieval:
  # Architecture de retrieval
  architecture: "hybrid_rerank"  # semantic | lexical | hybrid | hybrid_rerank

  # --- RECHERCHE SÉMANTIQUE ---
  semantic:
    enabled: true
    weight: 0.5           # Poids dans le score final (0.0 à 1.0)
    top_k: 20             # Nombre de résultats à récupérer
    similarity_threshold: 0.0  # Score minimum (0 = pas de filtre)
  
  # --- RECHERCHE LEXICALE (BM25) ---
  lexical:
    enabled: true
    weight: 0.5           # Poids dans le score final
    top_k: 20
    
    # Paramètres BM25
    algorithm: "bm25"     # bm25 | bm25+
    params:
      k1: 1.5             # Saturation du terme (1.2-2.0)
      b: 0.75             # Normalisation de longueur (0.5-0.8)
    
    # Preprocessing
    preprocessing:
      lowercase: true
      remove_stopwords: true
      stopwords_lang: "french"  # french | english | auto
      stemming: false

  # --- RERANKING ---
  rerank:
    enabled: true
    
    provider: "cohere"    # cohere | none
    model: "rerank-v3.5"
    api_key_env: "COHERE_API_KEY"
    
    # Paramètres
    top_n: 5              # Nombre final de résultats après rerank
    candidates: 40        # Nombre de candidats envoyés au reranker
    relevance_threshold: 0.0  # Score minimum après rerank

  # --- FUSION DES SCORES ---
  fusion:
    method: "reciprocal_rank_fusion"  # weighted_sum | reciprocal_rank_fusion
    
    # Pour weighted_sum : normaliser les scores avant fusion
    normalize_scores: true
    
    # Pour RRF : constante k
    rrf_k: 60

  # --- CONTEXTE FINAL ---
  context:
    # Nombre max de chunks dans le contexte
    max_chunks: 5
    
    # Taille max du contexte en tokens
    max_tokens: 4000
    
    # Déduplications de chunks similaires
    deduplication:
      enabled: true
      similarity_threshold: 0.95

# ============================================================================
# LLM - Modèles de langage
# ============================================================================
llm:
  # Modèle principal pour la génération de réponses
  primary:
    provider: "openai"  # openai | anthropic | ollama
    model: "gpt-4o-mini"
    api_key_env: "OPENAI_API_KEY"
    
    # Paramètres de génération
    params:
      temperature: 0.7
      max_tokens: 2000
      top_p: 0.95
    
    # Timeouts
    timeout: 60
    max_retries: 3
    
  # Modèle secondaire / fallback (optionnel)
  secondary:
    provider: "anthropic"
    model: "claude-sonnet-4-20250514"
    api_key_env: "ANTHROPIC_API_KEY"
    params:
      temperature: 0.5
      max_tokens: 2000

  # Modèle rapide pour les agents d'analyse
  fast:
    provider: "openai"
    model: "gpt-4o-mini"
    api_key_env: "OPENAI_API_KEY"
    params:
      temperature: 0.3
      max_tokens: 500

# ============================================================================
# AGENTS V1 - Système d'agents par défaut
# ============================================================================
# En V1, RAGKIT utilise 2 agents prédéfinis non personnalisables :
# 1. query_analyzer : Analyse la requête et décide si RAG est nécessaire
# 2. response_generator : Génère la réponse finale avec le contexte
# 
# Ces agents utilisent des prompts par défaut optimisés.
# La personnalisation avancée sera disponible en V2.
# ============================================================================

agents:
  # Mode V1 : agents par défaut uniquement
  mode: "default"  # default (V1) | custom (V2+)
  
  # --- AGENT 1 : Query Analyzer ---
  query_analyzer:
    # LLM à utiliser (référence à llm.*)
    llm: "fast"
    
    # Comportement de l'analyse
    behavior:
      # Doit-on toujours faire du RAG ou analyser d'abord ?
      always_retrieve: false  # false = analyse d'abord, true = RAG systématique
      
      # Types de requêtes détectées
      detect_intents:
        - "question"        # Question factuelle nécessitant RAG
        - "greeting"        # Salutation (pas de RAG)
        - "chitchat"        # Discussion générale (pas de RAG)
        - "out_of_scope"    # Hors périmètre (pas de RAG)
        - "clarification"   # Demande de clarification (pas de RAG)
      
      # Reformulation de la requête pour le retrieval
      query_rewriting:
        enabled: true
        num_rewrites: 1     # Nombre de reformulations (1-3)
    
    # Prompt système (personnalisable)
    # Variables disponibles : {query}, {conversation_history}
    system_prompt: |
      Tu es un analyseur de requêtes pour un système RAG.
      
      Ta tâche est d'analyser la requête utilisateur et de déterminer :
      1. L'intention de l'utilisateur (question, greeting, chitchat, out_of_scope, clarification)
      2. Si une recherche documentaire (RAG) est nécessaire
      3. Une reformulation optimisée de la requête pour la recherche (si RAG nécessaire)
      
      Règles :
      - Les salutations et bavardages ne nécessitent PAS de RAG
      - Les questions factuelles sur le domaine nécessitent du RAG
      - Si la requête est ambiguë, considère qu'elle nécessite du RAG
      
      Réponds UNIQUEMENT en JSON valide avec ce format :
      {
        "intent": "question|greeting|chitchat|out_of_scope|clarification",
        "needs_retrieval": true|false,
        "rewritten_query": "requête reformulée pour la recherche" | null,
        "reasoning": "courte explication de ta décision"
      }
    
    # Schéma de sortie attendu (pour validation)
    output_schema:
      type: "object"
      required: ["intent", "needs_retrieval"]
      properties:
        intent:
          type: "string"
          enum: ["question", "greeting", "chitchat", "out_of_scope", "clarification"]
        needs_retrieval:
          type: "boolean"
        rewritten_query:
          type: ["string", "null"]
        reasoning:
          type: "string"

  # --- AGENT 2 : Response Generator ---
  response_generator:
    # LLM à utiliser
    llm: "primary"
    
    # Comportement de génération
    behavior:
      # Citer les sources dans la réponse
      cite_sources: true
      citation_format: "[Source: {source_name}]"  # Format des citations
      
      # Admettre quand l'info n'est pas disponible
      admit_uncertainty: true
      uncertainty_phrase: "Je n'ai pas trouvé d'information à ce sujet dans la documentation."
      
      # Longueur de réponse
      max_response_length: null  # null = pas de limite (géré par max_tokens LLM)
      
      # Langue de réponse
      response_language: "auto"  # auto | fr | en | ...
    
    # Prompt système principal
    # Variables disponibles : {query}, {context}, {conversation_history}
    system_prompt: |
      Tu es un assistant expert qui répond aux questions en te basant sur une base documentaire.
      
      RÈGLES IMPORTANTES :
      1. Réponds UNIQUEMENT à partir du contexte documentaire fourni
      2. Si l'information n'est pas dans le contexte, dis-le clairement
      3. Cite tes sources en utilisant le format [Source: nom_du_document]
      4. Sois concis, précis et structuré
      5. Si plusieurs sources se contredisent, mentionne-le
      
      CONTEXTE DOCUMENTAIRE :
      {context}
      
      ---
      
      Réponds à la question de l'utilisateur en respectant les règles ci-dessus.
    
    # Prompt pour les réponses sans RAG (greeting, chitchat)
    no_retrieval_prompt: |
      Tu es un assistant conversationnel amical.
      Réponds de manière naturelle et concise.
      Si l'utilisateur pose une question qui nécessite des informations spécifiques, 
      indique-lui que tu peux l'aider à chercher dans la documentation.
    
    # Prompt pour les cas hors périmètre
    out_of_scope_prompt: |
      Tu es un assistant spécialisé dans un domaine précis.
      L'utilisateur a posé une question hors de ton périmètre de compétence.
      Explique poliment que tu ne peux pas répondre à cette question et 
      rappelle le type de questions auxquelles tu peux répondre.

  # --- PARAMÈTRES GLOBAUX DES AGENTS ---
  global:
    # Timeout pour chaque appel d'agent
    timeout: 30
    
    # Retry en cas d'erreur
    max_retries: 2
    retry_delay: 1  # secondes
    
    # Logging détaillé des agents
    verbose: false

# ============================================================================
# CONVERSATION - Gestion de l'historique
# ============================================================================
conversation:
  # Mémoire de conversation
  memory:
    enabled: true
    type: "buffer_window"  # buffer_window | summary | none
    
    # Pour buffer_window : nombre de messages à garder
    window_size: 10
    
    # Inclure l'historique dans le prompt
    include_in_prompt: true
    
  # Persistance (V1 : memory uniquement)
  persistence:
    enabled: false
    backend: "memory"  # memory (V1), redis/postgresql (V2+)

# ============================================================================
# CHATBOT - Interface utilisateur
# ============================================================================
chatbot:
  enabled: true
  
  # Type d'interface
  type: "gradio"  # gradio (V1)
  
  # Serveur
  server:
    host: "0.0.0.0"
    port: 8080
    share: false  # Créer un lien public Gradio
  
  # Apparence
  ui:
    title: "RAGKIT Assistant"
    description: "Posez vos questions sur la documentation"
    theme: "soft"  # soft | default | glass
    
    # Placeholder dans le champ de saisie
    placeholder: "Posez votre question..."
    
    # Exemples de questions (affichés au démarrage)
    examples:
      - "Qu'est-ce que [sujet principal] ?"
      - "Comment fonctionne [fonctionnalité] ?"
      - "Quelles sont les étapes pour [processus] ?"
  
  # Fonctionnalités
  features:
    # Afficher les sources utilisées
    show_sources: true
    
    # Afficher le temps de réponse
    show_latency: true

    # Streaming de la réponse (si supporté)
    streaming: false
    
    # Permettre le feedback utilisateur
    allow_feedback: false  # V2+
    
    # Export de la conversation
    allow_export: false  # V2+

# ============================================================================
# API - Endpoints REST
# ============================================================================
api:
  enabled: true
  
  # Serveur
  server:
    host: "0.0.0.0"
    port: 8000
    
  # CORS
  cors:
    enabled: true
    origins: ["*"]
    
  # Documentation automatique
  docs:
    enabled: true
    path: "/docs"

  # Streaming de réponses (SSE)
  streaming:
    enabled: false
    type: "sse"

# ============================================================================
# OBSERVABILITY - Logs et métriques
# ============================================================================
observability:
  # Logging
  logging:
    level: "INFO"  # DEBUG | INFO | WARNING | ERROR
    format: "text"  # text | json
    
    # Fichier de log (optionnel)
    file:
      enabled: false
      path: "./logs/ragkit.log"
      rotation: "daily"
      retention_days: 7

  # Métriques (V1 : basique)
  metrics:
    enabled: true
    # Métriques collectées
    track:
      - "query_count"
      - "query_latency"
      - "retrieval_latency"
      - "llm_latency"
      - "error_count"
